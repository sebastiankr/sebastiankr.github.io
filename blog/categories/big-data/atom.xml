<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Big Data | Sebastian Kropp]]></title>
  <link href="http://sebastian.kropponline.de/blog/categories/big-data/atom.xml" rel="self"/>
  <link href="http://sebastian.kropponline.de/"/>
  <updated>2014-09-21T16:00:44-04:00</updated>
  <id>http://sebastian.kropponline.de/</id>
  <author>
    <name><![CDATA[Sebastian Kropp]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[D3 Update Pattern on Nested Data]]></title>
    <link href="http://sebastian.kropponline.de/blog/2013/09/04/d3-update-pattern-on-nested-data/"/>
    <updated>2013-09-04T00:00:00-04:00</updated>
    <id>http://sebastian.kropponline.de/blog/2013/09/04/d3-update-pattern-on-nested-data</id>
    <content type="html"><![CDATA[<pre><code class="json ">{
    "name":"Application1",
    "logCounts": {
        "DEBUG": 8,
        "INFO": 12,
        "WARN": 32,
        "ERROR": 47,
        "FATAL": 5
    }
},
{
    "name":"Application2",
    "logCounts": {
...
</code></pre>

<script src="http://d3js.org/d3.v3.min.js"></script>


<div id="nestedUpdateExample"></div>




<script type="text/javascript">
(function () {
    "use strict";
    var data = [];
    data.push({ name: 'Application1', logCounts: { DEBUG: 8, INFO: 12, WARN: 32, ERROR: 47, FATAL: 5 } });
    data.push({ name: 'Application1', logCounts: { DEBUG: 1, INFO: 2, WARN: 3, ERROR: 4, FATAL: 5 } });

    function renderLoop(data) {
        if (data.length > 0) {
            var keys = d3.keys(data[0].logCounts);
            var table = d3.select("#nestedUpdateExample").append("table").attr("class","table table-striped");

            var header = table.append("thead").attr("id", "header").append("tr");

            header = header.selectAll("tr").data(function (d,i) {
                // Add "Application Name" to the header data
                var headerRowData = ["Application Name"];
                return headerRowData.concat(keys);
            });
            
            header.enter().append("td").text(function (d) {
                return d;
            });

            var tbody = table.append("tbody");
            var row = tbody.selectAll("tr").data(data);
            
            row.enter().append("tr");
            row.exit().remove();
            
            var cell = row.selectAll("td").data(function (d, i) {
                var rowdata = [{ name: "app", value: d.name }];
                rowdata = rowdata.concat(keys.map(function (keyname) {
                    return { name: keyname, value: d.logCounts[keyname] };
                }));
            return rowdata;
        });

        cell.text(function (count) {
            return count.value;
        });

        cell.enter().append("td").text(function (count) {
            return count.value;
        });
            
        } else {
            var message = d3.select("#nestedUpdateExample").append("p").text("Error: No data to display!").transition().style("color", "red");

            return;
        }
    };

    renderLoop(data);

    d3.select("#updateapp").on("click", function (d) {
        if (data.length > 0) {
            data[0].logCounts["INFO"] += 1;
            update(data);
        }
    });

    d3.select("#addapp").on("click", function (d) {
        data.push({ name: makeid(), logCounts: { DEBUG: 1, INFO: 2, WARN: 3, ERROR: 4, FATAL: 5 } });
        update(data);
    });

    d3.select("#removeapp").on("click", function (d) {
        data.pop();
        update(data);
    });

    function makeid() {
        var text = "";
        var possible = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789";

        for (var i = 0; i < 5; i++)
            text += possible.charAt(Math.floor(Math.random() * possible.length));

        return text;
    }
    ;
})();
</script>




<!--script type="text/javascript">

var data = [
    { name: &#8216;CARA&#8217;, logCounts: { DEBUG: 1, INFO: 2, WARN: 3, ERROR: 4, FATAL: 5 } }
];

var keys = d3.keys(data[0].logCounts);

if (data.length > 0) {
    var table = d3.select(&#8220;#nestedUpdateExample&#8221;).append(&#8220;table&#8221;);

    var header = table.append(&#8220;thead&#8221;).attr(&#8220;id&#8221;, &#8220;header&#8221;).append(&#8220;tr&#8221;);

    header.append(&#8220;td&#8221;).text(&#8220;app&#8221;);
    header.selectAll(&#8220;td&#8221;).data(keys).enter().append(&#8220;td&#8221;).text(function (d) {
        return d;
    });

    var tbody = table.append(&#8220;tbody&#8221;);
}

function update(data) {
    var row = tbody.selectAll(&#8220;tr&#8221;).data(data);

    row.enter().append(&#8220;tr&#8221;).append(&#8220;td&#8221;).text(data.name);

    row.exit().remove();

    var cell = row.selectAll(&#8220;td&#8221;).data(function (d, i) {
        var rowdata = [{ name: &#8220;app&#8221;, value: d.name }];
        rowdata = rowdata.concat(keys.map(function (keyname) {
            return { name: keyname, value: d.logCounts[keyname] };
        }));
        return rowdata;
    });

    cell.text(function (count) {
        return count.value;
    });

    cell.enter().append(&#8220;td&#8221;).text(function (count) {
        return count.value;
    });
}
;

update(data);

d3.select(&#8220;#updateapp&#8221;).on(&#8220;click&#8221;, function (d) {
    if (data.length > 0) {
        data[0].logCounts[&#8220;INFO&#8221;] += 1;
        update(data);
    }
});

d3.select(&#8220;#addapp&#8221;).on(&#8220;click&#8221;, function (d) {
    data.push({ name: makeid(), logCounts: { DEBUG: 1, INFO: 2, WARN: 3, ERROR: 4, FATAL: 5 } });
    update(data);
});

d3.select(&#8220;#removeapp&#8221;).on(&#8220;click&#8221;, function (d) {
    data.pop();
    update(data);
});

function makeid() {
    var text = &#8220;&#8221;;
    var possible = &#8220;ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789&#8221;;

    for (var i = 0; i < 5; i++)
        text += possible.charAt(Math.floor(Math.random() * possible.length));

    return text;
}
;
</script-->

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How Hadoop Does Not Scale]]></title>
    <link href="http://sebastian.kropponline.de/blog/2013/08/22/how-hadoop-does-not-scale/"/>
    <updated>2013-08-22T00:00:00-04:00</updated>
    <id>http://sebastian.kropponline.de/blog/2013/08/22/how-hadoop-does-not-scale</id>
    <content type="html"><![CDATA[<p><img class="right" src="/images/hadoop/hadoop-elephant.png" width="240" title="Hadoop Logo" ></p>

<p>We currently read a lot about how good Hadoop scales by mapping data and processes out to commodity nodes. To disappoint you right away, this post is not a general criticism of Hadoop. I do not even want to argue that Hadoop does not scale logarithmically. There are already a lot of papers that look at which algorithms are suited for MapReduce.</p>

<p>The purpose of this post is to look at slightly different aspects of scale, mostly aspects from an enterprise and financial viewpoint. The meaning of Hadoop changes rapidly. When I refer to Hadoop, I mean the traditional way of batch processing with MapReduce and not the amazing community of ambitious developers trying to find better ways for society to cope with the Big Data challenge.</p>

<h3>Why Hadoop does not scale</h3>

<!-- more -->


<h4>Management and Costs</h4>

<p>Let us start with the management problems associated with maintaining a lot of systems in a cluster. Every additional server puts a lot of pressure on an enterprise. It is not only capital expenditures and operational costs of maintaining the systems. There is also asset management, finance, budget planning, networking, increased energy costs, additional cooling, space requirements, and a lot of other things that need to be considered. Numbers for capital and operational expenses vary widely depending on the maturity of an organization. I would love to see studies on this topic. My guess would be that even the indirectly associated costs are substantial. Once you made this investment, it needs to be utilized. Batch jobs require resources only during their execution and create peak workloads. How do you effectively utilize the cluster when jobs finish? How do you plan the schedule and how can you forecast job utilization to even out the workloads?</p>

<h4>Virtualization and the Cloud</h4>

<p>With this utilization model, virtualization would be a great use case for Hadoop because it could offer the needed elasticity of compute resources. Virtualization tools would also be of great help in managing the cluster with provisioning automation and service management integration (self-service/ticketing of resource allocation). Problem is, that Hadoop was not designed for the cloud. It would be hard for YARN to discover the real topology of the underlying hardware, since virtualization hides that. On top of that, how much virtualization can a system take before it becomes inefficient? We have the hardware abstraction by the kernel, then the Java Virtual Machine and byte code. If you put that on top of another virtual layer which itself again has some abstractions, it is a long way for the instruction to reach the metal. Could system-level virtualization like Linux Containers or FreeBSD Jails help?</p>

<h4>Language</h4>

<p>Java and other garbage collected languages are not necessarily the best fit for big data. With managing a lot of data, developers will eventually have to deal with tuning the garbage collector. The memory model could be inefficient in other ways too. Java has no unsigned numbers, not many ways to affect memory alignment to optimize cache lines, no complex value types like <code>structs</code>, and a lot of heap allocation in general. The Java language and libraries often endorse memory sharing instead of messaging. Messaging constructs with share nothing memory models are considered easier to parallelize and scale. Map, reduce and filter concepts have had a place in functional languages for a long time. This is where map and reduce belongs, in a language. Unfortunately Java has very little support for functional concepts and Hadoop had to be attached on top of it, instead of being integrated in the language. I guess that became apparent and gave rise to higher level languages like Pig had to be introduced to the ecosystem. Now developers have to learn new languages which fragments the pool of developers even further and makes it hard to hire top talent or move that talent inside the enterprise. Additionally the community has to take on developing new languages, which is by no means an easy task. Maybe it might have been a good idea to learn from what functional language research and process algebra had to offer. Was there really a need to reinvent the wheel? Languages like Communicating Sequential Processes <a href="http://en.wikipedia.org/wiki/Communicating_sequential_processes">(CSP)</a> have channels that the MapReduce jobs could be abstracted upon. Why not use a functional language that takes care of distributing the workload efficiently itself, maybe with the help of parameterized functions where the parameters explain the infrastructure topology and expected volumes/loads. Libraries in modern object oriented languages like C# and C++ expand into this space with <code>lambdas</code> and libraries like <a href="http://en.wikipedia.org/wiki/Parallel_Extensions#Task_Parallel_Library">TPL</a>, <a href="http://en.wikipedia.org/wiki/Parallel_Patterns_Library">PPL</a>, and <a href="http://en.wikipedia.org/wiki/Threading_Building_Blocks">TBB</a>. Common to these libraries is that they make use of <code>promises</code> and <code>futures</code>, which essentially creates message channels between threads and enables truly amazing parallelism. Java will hopefully follow suit with Java 8.</p>

<h4>Scaling transistors vs scaling a cluster</h4>

<p><a href="http://en.wikipedia.org/wiki/Moore%27s_law">Moore&rsquo;s law</a> has proven to be good so far. Transistors are doubling every 18 month or so. Hadoop makes use of this exponential growth as well, but not nearly as good as it could. There is no build in way to make use of GPU or other hardware acceleration. Of course you can build that into your distributed functions, but it is not practical for a couple of reasons. In order to make use of the exponential growth, the environment needs to make efficient use of the die. Hadoop lacks that capability. If you want to scale better you make use of the nano-scale.
<blockquote><p>There&rsquo;s Plenty of Room at the Bottom</p><footer><strong>Richard Feynman <a href="http://en.wikipedia.org/wiki/There&amp;#39;s_Plenty_of_Room_at_the_Bottom">http://en.wikipedia.org/wiki/There&amp;#39;s_Plenty_of_Room_at_the_Bottom</a></strong></footer></blockquote></p>

<h4>Batch</h4>

<p>For a lot of use cases it is not the most efficient way to design your system with a sequence of batch processes. It is computationally very expensive to recalculate the whole batch just because some data changed. Often the delta that this new data represents can be integrated without a complete re-run. If your current analytics runs days and you need to go to hours, Hadoop is your tool. If you need to go from days to seconds, Hadoop is a dead end. In this case you need to go for an <a href="http://en.wikipedia.org/wiki/Event-driven_architecture">event-driven architecture</a>.</p>

<h4>Measure and compare scalability</h4>

<p>It is hard to measure scalability. The question, would it have been cheaper to build the system in another way, is rarely answered. Hey, you finally have a solution and it works. Who would build the same thing differently just to confirm hypotheses (except science)? All these arguments I made on &ldquo;Why Hadoop does not scale&rdquo; are hypotheses and would have to be proven and applied to your specific context. But keep in mind, the claims that it does scale are equally unproven and it all depends on which objective it is measured against.
But one strong indication for Hadoop’s suboptimal model is the probably biggest compute system called Bitcoin. Bitcoin is a very interesting case, because it is directly measured financially and different approaches compete within these metrics. How fast, how much energy, how much initial costs, operating costs? Well, is this a fair comparison? I would say yes and if to just to prove that Hadoop is not for ideal for everything. The block chain can be compared with HDFS, the miners are distributed on different nodes and they receive data and compute new data.</p>

<p>So if we look at Bitcoin we notice:</p>

<ul>
<li>it is not build on Hadoop</li>
<li>no JVM but native computation</li>
<li>it first utilized multicore architectures</li>
<li>then it harnessed GPU computation</li>
<li>GPU&rsquo;s were superseded by FPGAs</li>
<li>next was the design of optimized <a href="http://en.wikipedia.org/wiki/Application-specific_integrated_circuit">ASIC&rsquo;s</a></li>
<li>and only then pooling of nodes made sense</li>
</ul>


<h3>Why Hadoop does scale</h3>

<p>There are obviously reasons for why and how Hadoop <em>does</em> scale.</p>

<h4>Commodity nodes</h4>

<p>Sure, make use of the industry at a whole by piggy backing on advances driven by the big scales of commodity hardware. Supercomputers have long shifted to cluster commodity hardware. But commodity hardware includes more than desktops/servers. Do not forget all the nice gaming stuff like GPUs and physics accelerators, FPGA, etc. But as saw earlier, Hadoop, being a toy elephant himself, has problems playing with GPUs.</p>

<h4>Linux as an operating system</h4>

<p>Saving money by using a <em>&ldquo;free&rdquo;</em> OS is good, if you have a lot of nodes. On top of it, it will save a lot of time trying to understand Microsoft&rsquo;s licensing terms, which seems to double in complexity every 18 month. Additionally it is ideal to automate provisioning processes with UNIX based operating systems.</p>

<h4>No SQL databases</h4>

<p>Saving again on licenses. Most of analytics does not rely on ACID requirements anyway and eventual consistency is good enough. It is much easier to cluster these systems.</p>

<h4>Open Source</h4>

<p>Being able to sit on shoulders of a vibrant community is great. Enterprises can break their dependency from enterprise software vendors like IBM, Oracle or Microsoft. Additionally it ensures access to young talent in this area.</p>

<h3>Other References</h3>

<p><a href="https://wiki.apache.org/hadoop/Virtual%20Hadoop">Virtual Hadoop</a><br/>
<a href="http://www.theregister.co.uk/2010/09/24/google_percolator/">Google Percolator – global search jolt sans MapReduce comedown</a><br/>
<a href="http://www.nytimes.com/external/gigaom/2010/10/23/23gigaom-beyond-hadoop-next-generation-big-data-architectu-81730.html">Beyond Hadoop: Next-Generation Big Data Architectures</a><br/>
<a href="http://gigaom.com/2012/07/07/why-the-days-are-numbered-for-hadoop-as-we-know-it/">Why the days are numbered for Hadoop as we know it</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Big Data Visualization and D3]]></title>
    <link href="http://sebastian.kropponline.de/blog/2013/02/12/big-data-visualization-and-d3/"/>
    <updated>2013-02-12T00:00:00-05:00</updated>
    <id>http://sebastian.kropponline.de/blog/2013/02/12/big-data-visualization-and-d3</id>
    <content type="html"><![CDATA[<p><a href="http://d3js.org/">D3.js</a> is a great JavaScript library to visualize data. Visualization is an overlooked aspect of the Big Data picture. The real value of data is to gain an understanding and act accordingly. Visualization is a great way to make data understandable.</p>

<p>Additionally, as we look closer at &ldquo;Velocity&rdquo; as one of the 3Vs of Big Data, we need mechanisms to ingest and show events immediately. We need libraries that are compatible with our Event-driven architectures. Hey, we finally have WebSockets, let&rsquo;s use them! Maintaining report schedules and running batch processes is so 1990ies and a huge overhead.</p>

<p><img src="/images/d3/d3theme.jpg" title="D3 Theme" ></p>

<!-- more -->


<p>The D3 programming model is declarative and the developer is in full control to express data visually. It has many benefits over other libraries in this area:</p>

<ul>
<li>easy to show changes in data (updates and transitions)</li>
<li>powerful declarative model</li>
<li>easy to learn (lots of <a href="https://github.com/mbostock/d3/wiki/Gallery">examples</a>)</li>
<li>manipulates native DOM elements (reuse your knowledge in CSS, HTML, SVG)</li>
<li>very modular (you do not need everything)</li>
<li>composable (plays nice with others)</li>
</ul>


<h3>How does it compare to JQuery or MVC/MVVM concepts like Angular.js or Knockout.js?</h3>

<p>There is a substantial overlap between these three library concepts. The all have a different focus. But it is sometimes not easy to decide in which case to use which library. They can be used together of course, but here are some guidelines to help you decide.</p>

<h4>JQuery</h4>

<p>D3 can do a lot of what JQuery does in a similar way. Granted, it does not have such a rich gallery of plugins that make JQuery ideal for rapid development. But D3 can be used to attach event handlers to any DOM element or make AJAX data calls as well. JQuery is imperative and gets hard to maintain. I have not worked on a maturing D3 codebase as much as I had to fix bloated JQuery code. Because of D3&rsquo;s declarative nature I am sure that D3 code will not get messy as fast as JQuery code.</p>

<h4>Angular.js/Knockout.js</h4>

<p>Contrary to Angular or Knockout, D3 is not based on templates. These templates help to understand the initial DOM. D3 could be used with an initial DOM structure as well, it is better to completely construct the part of the DOM from scratch. This helps to create encapsulated modules. These modules could be used/attach on any host element in the DOM. It is probably easy to inject D3 functionality into the <a href="http://www.w3.org/TR/shadow-dom/">Shadow DOM</a>, once they are supported by all browsers. If data is changing the initial template completely, templates could be counterproductive. An example would be you start with a <code>table</code> and get data that requires you to switch over to a <code>SVG</code>. Angular.js offers obviously much more in terms of MVC encapsulation, site navigation/history etc. But it is amazing how much similarity and overlap there is. Core to all of them is the data binding and selectors. D3 is the only real declarative option. There are still a lot of for loops in Angular/Knockout templates. You could probably tweak Angular to do the same things as D3 and vice versa. But I would recommend using the different libraries in conjunction and use them according to their respective focus. And yes agreed, that is sometimes easier said than done, especially with Angular.js.</p>

<p>There are also some downsides to D3. There is this issues that all these libraries have with SEO. Or rather it is Google&rsquo;s and Bing’s problem that their crawlers are not indexing JavaScript generated DOMs. This has to change! Another issue with D3 is that it is sometimes hard to understand the side effects, especially if working with nested data. For example if you have multiple pie charts which need to appear and disappear, and these charts have changing data themselves. This is especially hard for programmers who have little experience with functional languages/concepts.</p>

<p>Hopefully I will be able to find some time to write a post about update patterns on nested data eventually to show you how this can be done with D3.</p>

<p><strong>[Update 05/22/2013]</strong>
<em>I finally got around to write something up. Have a look at <a href="/blog/2013/05/22/d3-update-pattern-on-nested-data">D3 Update Pattern on Nested Data</a>. This will help you to understand how to handle updates to nested data.</em></p>
]]></content>
  </entry>
  
</feed>
